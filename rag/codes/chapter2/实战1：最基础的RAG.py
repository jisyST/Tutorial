from lazyllm import Document, Retriever
import lazyllm
from datasets import load_dataset
import os


# RAG å®æˆ˜å‰–æ

# å°†LazyLLMè·¯å¾„åŠ å…¥ç¯å¢ƒå˜é‡
# import sys
# sys.path.append("/home/mnt/chenzhe1/Code/LazyLLM")
# # è®¾ç½®ç¯å¢ƒå˜é‡
# import os
# os.environ["LAZYLLM_SENSENOVA_API_KEY"] = ""
# os.environ["LAZYLLM_SENSENOVA_SECRET_KEY"] = ""

# 1.æ–‡æ¡£åŠ è½½ ğŸ“š
# RAG æ–‡æ¡£è¯»å–
# ä¼ å…¥ç»å¯¹è·¯å¾„
doc = Document("/home/mnt/chenzhe1/Code/Other/rag_master/")
print(f"å®é™…ä¼ å…¥è·¯å¾„ä¸ºï¼š{doc.manager._dataset_path}")

# ä¼ å…¥ç›¸å¯¹è·¯å¾„
# éœ€é…ç½®ç¯å¢ƒå˜é‡ï¼šexport LAZYLLM_DATA_PATH="/home/mnt/chenzhe1/Code"
# doc = Document("/paper/")

# 2.æ£€æŸ¥ç»„ä»¶ ğŸ•µ
# ä¼ å…¥ç»å¯¹è·¯å¾„
doc = Document("/home/mnt/chenzhe1/Code/Other/rag_master/")

# ä½¿ç”¨Retrieverç»„ä»¶ï¼Œä¼ å…¥æ–‡æ¡£docï¼ŒèŠ‚ç‚¹ç»„åç§°è¿™é‡Œé‡‡ç”¨å†…ç½®åˆ‡åˆ†ç­–ç•¥"CoarseChunk"ï¼Œç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°bm25_Chinese
retriever = Retriever(doc, group_name=Document.CoarseChunk, similarity="bm25_chinese", topk=3)

# è°ƒç”¨retrieverç»„ä»¶ï¼Œä¼ å…¥query
retriever_result = retriever("ä»€ä¹ˆæ˜¯é“ï¼Ÿ")

# æ‰“å°ç»“æœï¼Œç”¨get_content()æ–¹æ³•æ‰“å°å…·ä½“çš„å†…å®¹
print(retriever_result[0].get_content())

# 3.ç”Ÿæˆç»„ä»¶ ğŸ™‹
api_key = ''
llm_prompt = "ä½ æ˜¯ä¸€åªå°çŒ«ï¼Œæ¯æ¬¡å›ç­”å®Œé—®é¢˜éƒ½è¦åŠ ä¸Šå–µå–µå–µ"
llm = lazyllm.OnlineChatModule(source="sensenova", model="SenseChat-5").prompt(llm_prompt)
print(llm("æ—©ä¸Šå¥½ï¼"))
# >>> æ—©ä¸Šå¥½ï¼å–µå–µå–µ


# RAG çŸ¥è¯†åº“æ„å»º

# 1.æ•°æ®é›†ç®€ä»‹
'''CMRC 2018ï¼ˆChinese Machine Reading Comprehension 2018ï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œ
ç”¨äºä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£çš„è·¨åº¦æå–æ•°æ®é›†ï¼Œä»¥å¢åŠ è¯¥é¢†åŸŸçš„è¯­è¨€å¤šæ ·æ€§ã€‚
æ•°æ®é›†ç”±äººç±»ä¸“å®¶åœ¨ç»´åŸºç™¾ç§‘æ®µè½ä¸Šæ³¨é‡Šçš„è¿‘20,000ä¸ªçœŸå®é—®é¢˜ç»„æˆã€‚'''
dataset = load_dataset('cmrc2018')  # åŠ è½½æ•°æ®é›†
# dataset = load_dataset('cmrc2018', cache_dir='path/to/datasets') # æŒ‡å®šä¸‹è½½è·¯å¾„
print(dataset)

# 2.æ„å»ºçŸ¥è¯†åº“
def create_KB(dataset):
    '''åŸºäºæµ‹è¯•é›†ä¸­çš„contextå­—æ®µåˆ›å»ºä¸€ä¸ªçŸ¥è¯†åº“ï¼Œæ¯10æ¡æ•°æ®ä¸ºä¸€ä¸ªtxtï¼Œæœ€åä¸è¶³10æ¡çš„ä¹Ÿä¸ºä¸€ä¸ªtxt'''
    Context = []
    for i in dataset:
        Context.append(i['context'])
    Context = list(set(Context))  # å»é‡åè·å¾—256ä¸ªè¯­æ–™

    # è®¡ç®—éœ€è¦çš„æ–‡ä»¶æ•°
    chunk_size = 10
    total_files = (len(Context) + chunk_size - 1) // chunk_size  # å‘ä¸Šå–æ•´

    # åˆ›å»ºæ–‡ä»¶å¤¹data_kbä¿å­˜çŸ¥è¯†åº“è¯­æ–™
    os.makedirs("data_kb", exist_ok=True)

    # æŒ‰ 10 æ¡æ•°æ®ä¸€ç»„å†™å…¥å¤šä¸ªæ–‡ä»¶
    for i in range(total_files):
        chunk = Context[i * chunk_size: (i + 1) * chunk_size]  # è·å–å½“å‰ 10 æ¡æ•°æ®
        file_name = f"./data_kb/part_{i+1}.txt"  # ç”Ÿæˆæ–‡ä»¶å
        with open(file_name, "w", encoding="utf-8") as f:
            f.write("\n".join(chunk))  # ä»¥æ¢è¡Œç¬¦åˆ†éš”å†™å…¥æ–‡ä»¶

        # print(f"æ–‡ä»¶ {file_name} å†™å…¥å®Œæˆï¼")  # æç¤ºå½“å‰æ–‡ä»¶å·²å†™å…¥

# è°ƒç”¨create_KB()åˆ›å»ºçŸ¥è¯†åº“
create_KB(dataset['test'])
# å±•ç¤ºéƒ¨åˆ†çŸ¥è¯†åº“ä¸­çš„å†…å®¹
with open('data_kb/part_1.txt') as f:
    print(f.read())


# å®ç°æœ€åŸºç¡€çš„ RAG
# æ–‡æ¡£åŠ è½½
documents = lazyllm.Document(dataset_path="/home/mnt/chenzhe1/Code/Other/rag_master/")

# æ£€ç´¢ç»„ä»¶å®šä¹‰
retriever = lazyllm.Retriever(doc=documents, group_name="CoarseChunk", similarity="bm25_chinese", topk=3)

# ç”Ÿæˆç»„ä»¶å®šä¹‰
llm = lazyllm.OnlineChatModule(source="sensenova", model="SenseChat-5")

# prompt è®¾è®¡
prompt = '''You will act as an AI question-answering assistant and complete a dialogue task.
In this task, you need to provide your answers based on the given context and questions.'''
llm.prompt(lazyllm.ChatPrompter(instruction=prompt, extra_keys=['context_str']))

# æ¨ç†
query = "ä»€ä¹ˆæ˜¯é“ï¼Ÿ"
# å°†Retrieverç»„ä»¶å¬å›çš„èŠ‚ç‚¹å…¨éƒ¨å­˜å‚¨åˆ°åˆ—è¡¨doc_node_listä¸­
doc_node_list = retriever(query=query)
# å°†queryå’Œå¬å›èŠ‚ç‚¹ä¸­çš„å†…å®¹ç»„æˆdictï¼Œä½œä¸ºå¤§æ¨¡å‹çš„è¾“å…¥
res = llm({"query": query, "context_str": "".join([node.get_content() for node in doc_node_list])})

print(f'With RAG Answer: {res}')

# ç”Ÿæˆç»„ä»¶å®šä¹‰
llm_without_rag = lazyllm.OnlineChatModule(source="sensenova", model="SenseChat-5")
query = "ä»€ä¹ˆæ˜¯é“ï¼Ÿ"
res = llm_without_rag(query)
print(f'Without RAG Answer: {res}')
